{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Linear Algebra\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear algebra is a large, diverse subject with a surprisingly large array of applications.  It's the language of many mathematical and physical theories, machine learning, and efficient computation.  We'll focus on the practical aspects for the last two topics - very often, if you can express your computation through linear algebra rather than through a loop, you can take advantage of some highly optimized algorithms that will run much faster.\n",
    "\n",
    "Linear algebra lends itself to two interpretations: numerical and geometric.  We'll focus on the numerical, and only draw in the geometric when we need it.  If you were taking a formal class on linear algebra, you'd do the opposite and have the geometric interpretation foremost.  However, we're doing this in a few hours instead of a few weeks, so compromises need to be made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear equations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll follow convention and denote variables by $x$, $y$, $z$, $w$, and $t$ and pull in more letters if we need to.  An equation is _linear_ if all the variables appear as things like $2 x$ - i.e. no powers, no complicated functions, just the variable and a constant.  We'll be looking at systems of linear equations, where we have several of these we're trying to solve at the same time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Question_: Which of these equations are linear (again, $x$, $y$, $z$, $w$, and $t$ are variables and the rest are constants)?\n",
    " 1. $z = 0$\n",
    " 2. $\\pi w - e y = 4 x$\n",
    " 3. $x^2 + x - 2 = 0$\n",
    " 4. $\\sin(x) + \\cos(y) = 0$\n",
    " 5. $x y + x z = 1$\n",
    " 6. $x^2 = 2$\n",
    " 7. 4 (x + y) = 3 y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systems of linear equations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What sort of situation would give rise to a system of linear equations?  We'll use some contrived situations for convenience, but they come up quite often in machine learning.\n",
    "\n",
    "Simple example:\n",
    "\n",
    "$$3 x + 2.31 y = 2.2$$\n",
    "$$4 x - 1.2 y = 0$$\n",
    "\n",
    "Example from physics: analyzing a circuit\n",
    "\n",
    "![Sample circuit](http://www.webassign.net/serpop/ae21-9.gif)\n",
    "\n",
    "$$R_1 I_2 - R_2 I_1 = 10$$\n",
    "$$R_2 I_1 + R_3 I_3 = 0$$\n",
    "$$I_1 + I_2 - I_3 = 0$$\n",
    "\n",
    "As you might imagine, this notation gets clunky with more variables and equations.  For example, with four variables and four equations, we get:\n",
    "\n",
    "$$a_0 t + b_0 y + c_0 z + d_0 w = e_0$$\n",
    "$$a_1 t + b_1 y + c_1 z + d_1 w = e_1$$\n",
    "$$a_2 t + b_2 y + c_2 z + d_2 w = e_2$$\n",
    "$$a_3 t + b_3 y + c_3 z + d_3 w = e_3$$\n",
    "\n",
    "Note that I'm starting to make this more generic in terms of notation.  This is intentional.  But note there is a lot of repetition here.  We can make things a bit cleaner by switching to Matrix notation:\n",
    "\n",
    "$$\\begin{pmatrix}a_0 & b_0 & c_0 & d_0\\\\a_1 & b_1 & c_1 & d_1\\\\\n",
    "a_2 & b_2 & c_2 & d_2\\\\ a_3 & b_3 & c_3 & d_3\\end{pmatrix}\n",
    "\\begin{pmatrix}t\\\\y\\\\z\\\\w\\end{pmatrix} = \\begin{pmatrix}e_0\\\\e_1\\\\e_2\\\\e_3\\end{pmatrix}$$\n",
    "\n",
    "This says exactly the same thing, just in a different form.  But it introduces vectors and matrices, our two new objects.  In this case, we have two vectors and a matrix, which we'll write as\n",
    "\n",
    "$$\\mathbf{A} = \\begin{pmatrix}a_0 & b_0 & c_0 & d_0\\\\a_1 & b_1 & c_1 & d_1\\\\\n",
    "a_2 & b_2 & c_2 & d_2\\\\ a_3 & b_3 & c_3 & d_3\\end{pmatrix}\\qquad \\vec{x} = \\begin{pmatrix}t\\\\y\\\\z\\\\w\\end{pmatrix}\n",
    "\\qquad \\vec{e} = \\begin{pmatrix}e_0\\\\e_1\\\\e_2\\\\e_3\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nouns: Vectors and Matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors can be thought of as a collection of numbers, like a list.  They have a geometric meaning too, but we're not going to worry about that right now.  We take each row or element to be associated with a particular _dimension_ - a measurement of something.  Like a sensor value, a distance in a particular direction, a price, etc.  We can actually always treat these as if they were dimensions in space, i.e. $x$, $y$, and $z$ if there are three, etc.  Note that we have written these as \"column vectors\" - more on that in a moment.\n",
    "\n",
    "Side note: vectors in $\\mathbb{R}^n$: any vector can be interpreted as a location or direction in space, by simply taking the values to be the $x$, $y$, $z$, etc coordinates.  This is the beginning of the \"geometric\" interpretation that we're glossing over.\n",
    "\n",
    "If a vector is a list, a _matrix_ is a list of lists.  They don't just have a length, they have two dimensions.  The $\\mathbf{A}$ above is a 4 x 4 (\"four by four\") matrix.  The order is row x columns, so for\n",
    "\n",
    "$$\\mathbf{B} = \\begin{pmatrix}a_0 & b_0 & c_0\\\\a_1 & b_1 & c_1\\end{pmatrix}\\qquad\n",
    " \\mathbf{C} = \\begin{pmatrix}a_0 & b_0\\\\a_1 & b_1\\\\a_2 & b_2\\end{pmatrix}$$\n",
    " \n",
    "$\\mathbf{B}$ is a 2 x 3 and $\\mathbf{C}$ is a 3 x 2.  This also applies to vectors, so $\\vec{x}$ and $\\vec{e}$ above are both 4 x 1 - all vectors are also matrices.  These are \"column\" vectors, since they consist of a single column with multiple rows.  You can also have a 1 x 4 \"row\" vector\n",
    "\n",
    "$$\\vec{r} = \\begin{pmatrix}a&b&c&d\\end{pmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise_: what are the dimensions of the following matrices?\n",
    "\n",
    "$$\\mathbf{M_1} = \\begin{pmatrix}2 & 3 & 4\\\\7 & -1 & 9\\end{pmatrix}\\qquad\n",
    "\\mathbf{M_2} = \\begin{pmatrix}1 & 5\\\\ -2 & 7\\\\9 & 9\\end{pmatrix}\\qquad\n",
    "\\mathbf{M_3} = \\begin{pmatrix}4 & 7 & 2\\end{pmatrix}$$\n",
    "$$\\mathbf{M_4} = \\begin{pmatrix}4\\\\ 7 \\\\ 2\\end{pmatrix}\\qquad\n",
    "\\mathbf{M_5} = \\begin{pmatrix}2&1\\\\7&4\\end{pmatrix}\\qquad\n",
    "\\mathbf{M_6} = \\begin{pmatrix}1&0\\\\0&1\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbs: addition and multiplication\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined matrices, we'll need to figure out how to work with them.  We want to make sure that they behave the way we expect systems of equations to behave.  So first we should define multiplication, so that we have\n",
    "\n",
    "$$\\mathbf{A} \\vec{x} = \\vec{e}$$\n",
    "\n",
    "be the same as our equations.\n",
    "\n",
    "To make this all easier to write, we're going to make our notation more generic once again.  We'll define the elements of a matrix or vector to be the name of the vector, with subscripts denoting its place in the matrix/vector.  So we'll say\n",
    "\n",
    "$$\\mathbf{A} = \\begin{pmatrix}a_{00} & a_{01} & a_{02} & a_{03}\\\\a_{10} & a_{11} & a_{12} & a_{13}\\\\\n",
    "a_{20} & a_{21} & a_{22} & a_{23}\\\\ a_{30} & a_{31} & a_{32} & a_{33}\\end{pmatrix} \\qquad \\vec{x} = \n",
    "\\begin{pmatrix}x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3\\end{pmatrix}$$\n",
    "\n",
    "as the generic names of the elements, so we'll have $a_{01} = b_0$, etc.  The lowercase is a convention and not really necessary.\n",
    "\n",
    "Why bother?  Well, now we can talk about a generic matrix and vector, and we can write down a general form of multiplication and check if it is correct.  We, in general, define a matrix times a vector as resulting in another vector:\n",
    "\n",
    "$$\\vec{p} = \\mathbf{A} \\vec{x}$$\n",
    "\n",
    "giving\n",
    "\n",
    "$$p_{i} = \\sum_j a_{i j} x_j$$\n",
    "\n",
    "Let's work that out for our system of equations, noting that this operation is just tracing across the row in the matrix while tracing along the column of the vector.\n",
    "\n",
    "$$p_{0} = a_{00} x_0 + a_{01} x_1 + a_{02} x_2 + a_{03} x_3 = a_0 t + b_0 y + c_0 z + d_0 w $$\n",
    "\n",
    "Multiplying this all out, we get\n",
    "\n",
    "$$\\vec{p} = \\vec{e} \\quad \\implies \\quad \\begin{pmatrix}a_0 t + b_0 y + c_0 z + d_0 w \\\\\n",
    "a_1 t + b_1 y + c_1 z + d_1 w\\\\a_2 t + b_2 y + c_2 z + d_2 w\\\\a_3 t + b_3 y + c_3 z + d_3 w \\end{pmatrix} = \n",
    "\\begin{pmatrix}e_0\\\\e_1\\\\e_2\\\\e_3\\end{pmatrix}$$\n",
    "\n",
    "So we did get our system of equations back.  That's a relief.\n",
    "\n",
    "This turns out to be the general form of matrix multiplication: if\n",
    "\n",
    "$$\\mathbf{C} = \\mathbf{A}\\mathbf{B}$$\n",
    "\n",
    "then\n",
    "\n",
    "$$c_{i j} = \\sum_k a_{i k} b_{k j}$$\n",
    "\n",
    "This puts restrictions on what we can multiply: only those cases where the number of columns in the first matrix equals the number of rows in the second matrix work.  So we can multiply a (3 x 2) times a (2 x 4) to get a (3 x 4), but can't reverse them to multiply a (2 x 4) times a (3 x 2) - this won't work.  This means that matrix multiplication doesn't commute!  So even if the shapes allow it, in general\n",
    "\n",
    "$$\\mathbf{A} \\mathbf{B} \\neq \\mathbf{B} \\mathbf{A}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise_: which of the matrices from the last exercise can be multiplied, and in what order?  What is the shape of their product?\n",
    "\n",
    "_Part two_: pick a few compatible pairs and multiply them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should tackle addition.  We can add two equations like so:\n",
    "\n",
    "$$(a x + b y) + (c x + d y) = (a + c) x + (b + d) y$$\n",
    "\n",
    "Following that logic, we should have\n",
    "\n",
    "$$\\begin{pmatrix}a_0 x + b_0 y\\\\a_1 x + b_1 y\\\\a_2 x + b_2 y\\end{pmatrix} + \n",
    "\\begin{pmatrix}c_0 x + d_0 y\\\\c_1 x + d_1 y\\\\c_2 x + d_2 y\\end{pmatrix} = \n",
    "\\begin{pmatrix}(a_0 + c_0) x + (b_0 + d_0) y\\\\(a_1 + c_1) x + (b_1 + d_1) y\\\\(a_2 + c_2) x + \n",
    "(b_2 + c_2) y\\end{pmatrix}$$\n",
    "\n",
    "rewriting in our matrix notation\n",
    "\n",
    "$$\\begin{pmatrix}a_0 & b_0\\\\a_1 & b_1\\\\a_2 & b_2\\end{pmatrix} \\begin{pmatrix}x\\\\y\\end{pmatrix} + \n",
    "\\begin{pmatrix}c_0 & d_0\\\\c_1 & d_1\\\\c_2 & d_2\\end{pmatrix} \\begin{pmatrix}x\\\\y\\end{pmatrix} = \n",
    "\\begin{pmatrix}(a_0 + c_0) & (b_0 + d_0)\\\\(a_1 + c_1) & (b_1 + d_1)\\\\(a_2 + c_2) & (b_2 + c_2)\\end{pmatrix}\n",
    "\\begin{pmatrix}x\\\\y\\end{pmatrix}$$\n",
    "\n",
    "So matrix addition is just adding corresponding elements.  That worked out pretty well.  Though we should note that this only makes sense for matrices that have the same shape.  So you can add two (3 x 2)'s, but not a (3 x 2) and a (2 x 3).  But at least it commutes, so\n",
    "\n",
    "$$\\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Question_: How does this extend to vectors?\n",
    "\n",
    "_Exercise_: Take some of our example matrices above and add them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalar product and orthogonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A special case of matrix multiplication is multiplying two vectors.  Of course, we can't _directly_ multiply two 4 x 1 vectors - their shape is incompatible.  Instead, we'll first introduce the *transpose*, which simply swaps rows for columns.  It's noted by a upper case $T$ as an exponent.  Working with our matrices from before:\n",
    "\n",
    "$$\\mathbf{B} = \\begin{pmatrix}a_0 & b_0 & c_0\\\\a_1 & b_1 & c_1\\end{pmatrix}\\qquad\n",
    " \\mathbf{C} = \\begin{pmatrix}a_0 & b_0\\\\a_1 & b_1\\\\a_2 & b_2\\end{pmatrix}$$\n",
    "\n",
    "$$\\mathbf{B}^T = \\begin{pmatrix}a_0 & a_1\\\\b_0 & b_1\\\\c_0 & c_1\\end{pmatrix}\\qquad\n",
    " \\mathbf{C}^T = \\begin{pmatrix}a_0 & a_1 & a_2\\\\b_0 & b_1 & b_2\\end{pmatrix}$$\n",
    " \n",
    "Another way to think about it is that we're swapping the order of the element indices, so\n",
    "\n",
    "$$a_{ij} \\rightarrow a_{ji}$$\n",
    "\n",
    "Note that this changes the shape of the matrix.  In particular, it's going to turn column vectors into row vectors and vice versa.  Now we can make them compatible!  We'll define the inner or dot product of two vectors (of the same shape) as\n",
    "\n",
    "$$\\mathrm{dot}(\\vec{a},\\vec{b}) = \\vec{a}\\cdot\\vec{b} = \\vec{a}^T \\vec{b}$$\n",
    "\n",
    "The resulting object will be a 1 x 1, i.e. a scalar (just a number).  As an example\n",
    "\n",
    "$$\\vec{x}\\cdot\\vec{e} = \\begin{pmatrix}t & y & z & w\\end{pmatrix}\n",
    "\\begin{pmatrix}e_0\\\\e_1\\\\e_2\\\\e_3\\end{pmatrix} = e_0 t + e_1 y + e_2 z + e_3 w$$\n",
    "\n",
    "An even more special case is the dot product of a vector with itself.  This gives a scalar measurement of the vector, and gives rise to the vector norm (note that we'll actually define the square of the norm)\n",
    "\n",
    "$$\\lVert \\vec{v} \\lVert^2 = \\vec{v}\\cdot\\vec{v}$$\n",
    "\n",
    "If these vectors are in $\\mathbb{R}^n$, then this is the traditional Euclidean distance given by the Pythagorean theorem\n",
    "\n",
    "Under our interpretation of vector columns as being directions in space, we can look at the concept of _orthogonality_.  Two directions are orthogonal if they are perpendicular, that is two lines drawn in the respective directions cross at a right angle.  In 2-D, we might choose the $x$ and $y$ axes, or the two vectors at 45 degrees from them, or similar.  Using those two as an example, let's look at their dot products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.axhline(y=0, color='0.5')\n",
    "ax.axvline(x=0, color='0.5')\n",
    "plt.quiver([0, 0], [0, 0], [0, 1], [1, 0], angles='xy', scale_units='xy', scale=1, color='r')\n",
    "plt.quiver([0, 0], [0, 0], [1, 1], [1, -1], angles='xy', scale_units='xy', scale=1, color='b')\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(-2, 2)\n",
    "plt.title('(0,1), (1,0), (1,1), (1,-1) as vectors')\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{pmatrix}1\\\\0\\end{pmatrix}\\cdot\\begin{pmatrix}0\\\\1\\end{pmatrix} = 0\\qquad\n",
    "\\begin{pmatrix}1\\\\1\\end{pmatrix}\\cdot\\begin{pmatrix}1\\\\-1\\end{pmatrix} = 0$$\n",
    "\n",
    "This turns out to hold true in general.  If two vectors are orthogonal, their dot product is zero.  This is actually how orthogonality is _defined_ in more complicated spaces - two vectors are orthogonal if and only if their dot product is zero.  This is so important we'll write it again\n",
    "\n",
    "$$\\vec{a} \\cdot \\vec{b} = 0 \\qquad \\iff \\qquad \\vec{a}\\; \\mathrm{and}\\; \\vec{b}\\ \n",
    "\\mathrm{are\\ orthogonal}$$\n",
    "\n",
    "We actually go one step further, and use this to define the angle between two vectors\n",
    "\n",
    "$$\\vec{a} \\cdot \\vec{b} = \\lVert \\vec{a} \\lVert\\,\\lVert \\vec{b} \\lVert\\,\\cos(\\theta)$$\n",
    "\n",
    "Why is orthogonality so important?  It's because orthogonal vectors carry independent information from each other, just like our $x$ and $y$ directions.  They are also extremely important in the geometric interpretation, as they define different directions in a space.\n",
    "\n",
    "_Exercise_: Find the dot products between these vectors, and their norms.  What angles do they make with each other?\n",
    "$$\\vec{v}_1 = \\begin{pmatrix}1\\\\0\\\\1\\end{pmatrix} \\qquad \n",
    "\\vec{v}_2 = \\begin{pmatrix}2\\\\0\\\\2\\end{pmatrix} \\qquad\n",
    "\\vec{v}_3 = \\begin{pmatrix}1\\\\-1\\\\1\\end{pmatrix} \\qquad\n",
    "\\vec{v}_4 = \\begin{pmatrix}0\\\\1\\\\1\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear independence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This idea of independent information is more than just vectors being orthogonal.  We say a set of vectors is _linearly independent_ if we can't write any of them as a linear combination of the others.  Orthogonal vectors are automatically linearly independent, but others can be as well.  In general, we say vectors $\\vec{v}_i$ are linearly dependent if we can find a set of constants $c_i$ such that\n",
    "\n",
    "$$\\sum_i c_i \\vec{v}_i = c_0 \\vec{v}_0 + c_1 \\vec{v}_1 + ... + c_n \\vec{v}_n = 0$$\n",
    "\n",
    "As example of dependent vectors is\n",
    "\n",
    "$$\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix} \\qquad \\begin{pmatrix}1\\\\1\\\\-1\\end{pmatrix}\n",
    "\\qquad \\begin{pmatrix}1\\\\1\\\\0\\end{pmatrix}$$\n",
    "\n",
    "since we can sum the first two to produce twice the second, like so:\n",
    "\n",
    "$$\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix} + \\begin{pmatrix}1\\\\1\\\\-1\\end{pmatrix} =\n",
    "\\begin{pmatrix}2\\\\2\\\\0\\end{pmatrix}\n",
    "= 2\\begin{pmatrix}1\\\\1\\\\0\\end{pmatrix}$$\n",
    "\n",
    "However, this very similar set is independent.\n",
    "\n",
    "$$\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix} \\qquad \\begin{pmatrix}1\\\\1\\\\-1\\end{pmatrix}\n",
    "\\qquad \\begin{pmatrix}1\\\\-1\\\\0\\end{pmatrix}$$\n",
    "\n",
    "In a system with $n$ linearly independent vectors, we can define exactly $n$ orthogonal directions.  This is the _dimension_ of the system (yes, that term is overloaded).  It's a measure of the information content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Transformations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to take a small detour for a moment, and talk about linear functions.  We talked about these a little at the beginning - in 1-D they're functions that look like\n",
    "\n",
    "$$f(x) = a x$$\n",
    "\n",
    "where $a$ is a constant.  But we can also define them in more dimensions: they'll just take a vector in and we'll get a vector out (they don't need to be the same length).  An example from 3-D to 2-D is\n",
    "\n",
    "$$\\vec{f}(\\vec{x}) = \\begin{pmatrix}a_x x + a_y y + a_z z\\\\b_x x + b_y y + b_z z\\end{pmatrix}$$\n",
    "\n",
    "where $a_i$ and $b_i$ are all constants.  But this should look awfully familiar - it's just\n",
    "\n",
    "$$\\begin{pmatrix}a_x&a_y&a_z\\\\b_x&b_y&b_z\\end{pmatrix}\\begin{pmatrix}x\\\\y\\\\z\\end{pmatrix}$$\n",
    "\n",
    "Any such linear function can be expressed as a matrix - this is, of course, by design, it's how we got them in the first place.  They have a special property, though, that we can take advantage of\n",
    "\n",
    "$$\\vec{f}(\\alpha \\vec{x} + \\beta \\vec{y}) = \\alpha\\,\\vec{f}(\\vec{x}) + \n",
    "\\beta\\,\\vec{f}(\\vec{y})$$\n",
    "\n",
    "This property is so fundamental it actually defines what is a linear transformation in general.\n",
    "\n",
    "_Exercise_: Show this works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvectors and eigenvalues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every matrix turns out to have special vectors that it doesn't really change.  These are called \"eigenvectors\" (eigen is German for \"good\", they're also called \"characteristic vectors\"), and we define them simply as vectors where\n",
    "\n",
    "$$\\mathbf{M} \\vec{v} = \\lambda \\vec{v}$$\n",
    "\n",
    "where $\\lambda$ is just a constant, called the eigenvalue for that vector.  Note that multiple vectors can have the same value, but if they do so does any linear combination of them.  Note that these can be complex, even for a real matrix.  Let's do an example, where we have worked out the eigenvectors beforehand.\n",
    "\n",
    "$$\\begin{pmatrix}1&-1\\\\1&1\\end{pmatrix}\\begin{pmatrix}1\\\\i\\end{pmatrix} = \n",
    "\\begin{pmatrix}1-i\\\\1+i\\end{pmatrix} = (1-i)\\begin{pmatrix}1\\\\i\\end{pmatrix}\\qquad\n",
    "\\begin{pmatrix}1&-1\\\\1&1\\end{pmatrix}\\begin{pmatrix}1\\\\-i\\end{pmatrix} = \n",
    "\\begin{pmatrix}1+i\\\\1-i\\end{pmatrix} = (1+i)\\begin{pmatrix}1\\\\-i\\end{pmatrix}$$\n",
    "\n",
    "In a more impressive \"it turns out\", every matrix has exactly as many of these vectors as it has dimensions, and knowing these vectors plus the $\\lambda$ for each tells you everything you can know about the matrix.  This \"eigensystem\" lets you reconstruct the original matrix, but it offers more.  The eigenvectors form a linearly independent set of vectors (if they all have different eigenvalues).  Knowing the eigenvalues lets you calculate many matrix properties easily.  The eigenvectors let you rewrite the matrix in a much simpler form.  And these eigenvectors are \"fixed points\" of the corresponding linear transformation.\n",
    "\n",
    "There is a general way to compute them, but it's relatively advanced.  For now, we can use a computer program if we need them, but their mere existence actually tells us a lot later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2017 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
