{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model, utils, preprocessing\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear and Logistic Regression: What this is all about\n",
    "<!-- requirement: images/linear_regression_error.gif -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*&copy; The Data Incubator*\n",
    "\n",
    "You might be familiar with term *linear regression*. It is a mathematical technique for identifying linear relationships in data.\n",
    "\n",
    "We are going to explore Linear Regression using a data set called the *California Housing Data*. This is a useful data set for building and benchmarking models. The data set contains aggregated data on housing values and characteristics by census block. Researchers can study the data to predict the median value of homes based on the other variables.\n",
    "\n",
    "This is a standard data set and comes with Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "cali_data = fetch_california_housing()\n",
    "X = cali_data['data']\n",
    "y = cali_data['target']\n",
    "print(cali_data['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the data to a DataFrame to make it easier to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = cali_data['feature_names']\n",
    "\n",
    "data_dict = dict(zip(names, ['Median income', 'House age', 'Average # of rooms', 'Average # of bedrooms', 'Population', 'Average occupancy', 'Latitude', 'Longitude']))\n",
    "\n",
    "cali_df = DataFrame(cali_data['data'], columns=names)\n",
    "\n",
    "home_values = Series(cali_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cali_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can familiarize ourselves with the data by plotting it. Let's have some fun exploring data using `IPython` widgets.\n",
    "\n",
    "Experiment with the dropdown to plot each column vs. Median home value.\n",
    "\n",
    "**Question:** Which columns seem to have somewhat of a linear relationship with home values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "\n",
    "def cali_plot(column):\n",
    "    plt.plot(cali_df[column], home_values, '.')\n",
    "    plt.xlabel(data_dict[column])\n",
    "    plt.ylabel('Home Price')\n",
    "\n",
    "dropdown_values = {\"{0}: {1}\".format(k, v):k for k, v in data_dict.items()}\n",
    "\n",
    "widgets.interact(cali_plot, column=dropdown_values);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median income feature in particular seems to have a linear relationship with home price. We can attempt to visualize that by adding a straight line to the chart.\n",
    "\n",
    "**Question:** In the below figure, which line seems to best \"fit\" the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cali_df['MedInc'], home_values, '.')\n",
    "plt.plot([0, 9], [0, 5], '-', color='darkorange')\n",
    "plt.plot([.5, 8], [0, 5], '-', color='limegreen',)\n",
    "plt.plot([2, 5], [0, 5], '-', color='red')\n",
    "\n",
    "plt.xlabel(data_dict['MedInc'])\n",
    "plt.ylabel('Home Price');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red one looks like a poor fit to me but I can't decide if the green or orange one is the better.\n",
    "\n",
    "But rather than eye-balling lines on charts, wouldn't it be great if there was a way to pick the \"best\" line that fits  the data?\n",
    "\n",
    "Linear regression is just that process. It is a mathematical process for measuring a line's \"error\" with respect to the data and picking the line that minimizes that error. It can describe the linear relationship between a set of numbers (the independent variables) and the dependent variable.\n",
    "\n",
    "In this case the independent variable is the \"average number of rooms per dwelling\" and the dependent variable is the Home Price. The independent variables need not be limited to one variable. A complex model may have hundreds or more dependent variables!\n",
    "\n",
    "Let's try Linear Regression to find the best fitting line. Is this result what you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = linear_model.LinearRegression(fit_intercept=True)  # fit_intercept=True is the default value\n",
    "linreg.fit(cali_df[['MedInc']], home_values)\n",
    "x = np.linspace(-1, 15).reshape(-1,1)\n",
    "\n",
    "plt.plot(cali_df['MedInc'], home_values, '.')\n",
    "plt.plot(x, linreg.predict(x), '-')\n",
    "plt.ylim(0, 5)\n",
    "plt.xlim(0, 15)\n",
    "plt.xlabel(data_dict['MedInc'])\n",
    "plt.ylabel('Home Price');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dig deeper into how linear regression works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the basic picture of linear regression errors:\n",
    "\n",
    "![$L^1$ versus $L^2$ regularization](images/linear_regression_error.gif)\n",
    "\n",
    "Linear Regression is perhaps the simplest linear model $f(X_{j \\cdot}) = \\sum_i \\beta_i X_{ji}$.  The error model assumes the $y_j$'s are independent and normally distributed around $X_{ji} \\cdot \\beta_i$ with standard deviation $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood and cost functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we knew that the correct model was given by some $\\beta_i$.  Given the above assumption about the error model, the probability of measuring $y_j$ is simply\n",
    "\n",
    "$$ P(y_j \\mid \\beta_i) = \\prod_j \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp \\left[-\\left( \\frac{X_{ji} \\cdot \\beta_i - y_j}{2 \\sigma} \\right)^2 \\right] \\,.$$\n",
    "\n",
    "However, we don't know the $\\beta_i$.  Instead we want to find them, given the $y_j$, by finding the $\\beta_i$ that maximize $P(\\beta_i \\mid y_j)$.  Thanks to Bayes' Rule, we know\n",
    "\n",
    "$$ P(\\beta_i \\mid y_j) = P(y_j \\mid \\beta_i) \\frac{P(\\beta_i)}{P(y_j)} \\,.$$\n",
    "\n",
    "We know the first term on the right, and $P(y_j)$ is independent of $\\beta_i$, leaving only $P(\\beta_i)$ unknown.  In linear regression, we suppose we have no *a priori* knowledge of the expected coefficients and take $P(\\beta_i)$ to be constant as well.  Thus, the most probable model is determined by maximizing the likelihood function\n",
    "\n",
    "$$ L(\\beta) = \\prod_j \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp \\left[-\\left( \\frac{X_{ji} \\cdot \\beta_i - y_j}{2 \\sigma} \\right)^2 \\right] \\propto P(\\beta_i \\mid y_j) \\,.$$\n",
    "\n",
    "Since $\\log$ is monotonic, we can also maximize the log-likelihood.  A few calculations show us that the negative log-likelihood (up to a linear transformation) is\n",
    "\n",
    "$$- \\log(L(\\beta)) \\sim \\| y - X \\beta \\|^2\\,.$$\n",
    "\n",
    "Here, $\\| z \\| = \\| z \\|_2 = \\sum_i |z_i|^2 $ is the $L^2$ norm.  The objective is to minimize this quadratic:\n",
    "\n",
    "$$ \\min_\\beta \\| y - X \\beta \\|^2\\,.$$\n",
    "\n",
    "Of course, this is the familiar expression for linear regression.  We could minimize $\\beta$ via gradient descent, but it turns out that the solution has a closed form, \n",
    "\n",
    "$$ X \\hat \\beta = y\\,, $$\n",
    "\n",
    "or\n",
    "\n",
    "$$ \\hat \\beta = (X^T X)^{-1} X^T y\\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this is used in practice. We will do a Linear Regression as before, but will shuffle the data first.\n",
    "\n",
    "**Question:** Why is shuffling the data a good idea?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xraw, y = utils.shuffle(cali_df, home_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data with a train-test split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use TRAIN data to fit our model, and we predict on TEST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(Xraw, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation allows us to *train* the model on a subset of the data and later *test* the model on a different subset of data.\n",
    "\n",
    "We shuffled the data to make sure that cross validation split is being done randomly. Sometimes the ordering of the data set you start out with is not random. Watch out for this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print Xraw.shape\n",
    "print X_train.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to scale our data so that it has a mean of zero and a variance of one. Scaling the data is helpful when the input variables have different magnitudes and ranges. For example in the California data set the relative scale of the `CHAS` and `NOX` columns is very different from that of the TAX column.\n",
    "\n",
    "Scikit-learn has a transformer called `StandardScaler` that does exactly what we need.\n",
    "\n",
    "Observe that we `fit` the `StandardScaler` with only the training data and then `transform` both the training and test data. This ensures that the `StandardScaler` is not scaling based on information from the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can validate that the mean and variance is scaled as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_means = X_train.mean(axis=0).to_frame('mean')\n",
    "\n",
    "train_means['var'] = X_train.var(axis=0)\n",
    "train_means['scaled_mean'] = X_train_scaled.mean(axis=0)\n",
    "train_means['scaled_var'] = X_train_scaled.var(axis=0)\n",
    "\n",
    "train_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test data statistics look similar but observe the `scaled_mean` values are small but not equal to zero. This is expected because the `StandardScaler` was fit with only the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_means = X_test.mean(axis=0).to_frame('mean')\n",
    "test_means['var'] = X_test.var(axis=0)\n",
    "test_means['scaled_mean'] = X_test_scaled.mean(axis=0)\n",
    "test_means['scaled_var'] = X_test_scaled.var(axis=0)\n",
    "\n",
    "test_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model to the training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is ready, create a Scikit-learn `LinearRegression` model and fit the model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = linear_model.LinearRegression()\n",
    "\n",
    "linreg.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the fitted model to make predictions with the test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed in the test data to make out \"out-of-sample\" predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the model parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do those numbers mean?\n",
    "\n",
    "They are parameters in the following equation. This equation defines the model's linear relationship between the the data attributes and the cost of homes in California."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"prediction = \" +\n",
    "       \"{0} +\\n\".format(linreg.intercept_) +\n",
    "       \" +\\n\".join([\"{1} * {0}\".format(n, f) for n, f in zip(names, linreg.coef_)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model is making predictions, it throws the test data in that equation and calculates the predicted value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Error Measurements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the $R^2$ of the fitted model on the training data.\n",
    "\n",
    "This number can be compared to other models on the same data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.score(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the $R^2$ of the fitted model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression results can be improved with Regularization techniques.\n",
    "\n",
    "Regularization adds a \"cost\" to the optimization, penalizing larger coefficient values. This can help combat overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression adds a penalty to the Linear Regression optimization that is proportional to the sum of the squared parameter values, like this:\n",
    "\n",
    "$$- \\log(L(\\beta)) \\sim \\| y - X \\beta \\|^2 + \\alpha \\| \\beta \\|^2\\,.$$\n",
    "\n",
    "This reduces the occurrence of extreme positive or negative values sometimes this improves out-of-sample model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model to the training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, fit model with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=5.0)\n",
    "\n",
    "ridge.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the fitted model to make predictions with the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the model parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the sum of the squared model parameters is smaller than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print sum(linreg.coef_ ** 2)\n",
    "print sum(ridge.coef_ ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Error Measurements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $R^2$ of the fitted model on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.score(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the out-of-sample test data. This is marginally better than the first linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `RidgeCV`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RidgeCV()` is just like `Ridge()` but with cross-validation built in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "ridgecv = RidgeCV(alphas=(0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 3.5, 5.0, 7.5, 10.0))\n",
    "\n",
    "ridgecv.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the fitted model to make predictions with the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgecv.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the model parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgecv.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the `alpha` value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgecv.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Error Measurements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $R^2$ of the fitted model on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgecv.score(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the out-of-sample test data. This is marginally better than the first linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgecv.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso is like ridge regression but has the ability to automatically select features.  The objective function to minimize is\n",
    "\n",
    "$$ \\frac{1}{2 n} \\| y - X^T \\beta \\|^2 + \\alpha \\|\\beta\\|_1 $$\n",
    "\n",
    "where $\\|\\beta\\|_1 = \\sum_i |\\beta_i| $ is the $L^1$ norm (sum of the absolute values) of $\\beta$ and $n$ is the number of samples. Lasso has a feature selection property where many weights on features are zero (i.e. those features are not selected)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.2)\n",
    "\n",
    "lasso.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the fitted model to make predictions with the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that some of the parameters are zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means some of the attributes are essentially excluded from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"prediction = \" +\n",
    "       \"{0} +\\n\".format(lasso.intercept_) +\n",
    "       \" +\\n\".join([\"{1} * {0}\".format(n, f) for n, f in zip(names, lasso.coef_)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Error Measurements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $R^2$ of the fitted model on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso.score(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Logistic Regression, the values of $y$ are categorical ($0$ or $1$) and assumed to be distributed binomially.  We assume that the probability $p(X_{j\\cdot})$ that $y = 1$ is related to $X$ via the logit function:\n",
    "\n",
    "$$ \\mbox{logit }(p(X_{j\\cdot})) = \\log \\frac{p(X_{j\\cdot})}{1-p(X_{j\\cdot})} = X_{ji} \\cdot \\beta_i\\,. $$\n",
    "\n",
    "Notice that the logit function $\\log \\frac{x}{1-x}$ is just the log odds and maps the real numbers $[0,1]$ to $\\mathbb{R}$.\n",
    "\n",
    "Below is a plot of the logit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lx = np.linspace(0.001,0.999,100)\n",
    "ly = np.log(lx/(1-lx))\n",
    "plt.plot(lx,ly)\n",
    "plt.xlabel('$p$')\n",
    "plt.ylabel(r'$\\mathrm{logit}(p)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be more clear to invert this to get an expression for $p(X_{j\\cdot})$:\n",
    "\n",
    "$$ p(X_{j\\cdot}) = \\frac{\\exp\\left( X_{ji} \\cdot \\beta_i\\right)}{1 + \\exp\\left( X_{ji} \\cdot \\beta_i\\right)} $$\n",
    "\n",
    "The input can vary over the entire real numbers, but the output is always a valid probability between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lx = np.linspace(-10, 10)\n",
    "ly = np.exp(lx) / (1 + np.exp(lx))\n",
    "plt.plot(lx, ly)\n",
    "plt.xlabel(r'$X_{ji} \\cdot \\beta_i$')\n",
    "plt.ylabel(r'$P(X_{j\\cdot})$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we'll try to predict whether the home price is greater than or less than $25K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat_train = y_train > 2.\n",
    "y_cat_test = y_test > 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "logistic.fit(X_train_scaled, y_cat_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the fitted model to make predictions with the test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the predictions are now True/False values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Error Measurements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R^2$ doesn't make sense for logistic regression, so this uses % accuracy instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic.score(X_train_scaled, y_cat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic.score(X_test_scaled, y_cat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass classification problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have talked about Two-Class classification in the context of Logistic Regression.  But what if we have more than two classes?  There are generally two strategies to \"bootstrap\" a binary classifier to a multi-class classifier: \n",
    "1. **One-versus-All**: For each class $k=1,\\ldots,K$, build a binary classifier for all points with label $y = k$ versus $y \\neq k$.\n",
    "1. **All-versus-All**: For each class $k \\neq k'$, construct a binary classifier to distinguish between class $k$ and $k'$.\n",
    "\n",
    "[Scikit](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) uses One-versus-All for Multi-class Logistic Regression.\n",
    "\n",
    "If $f_k(x)$ is the predictor for class $k$, the probability of class $k$ is just the normalized predictions,\n",
    "\n",
    "$$ p_k = \\frac{f_k(x)}{\\sum_k f_k(x)}$$\n",
    "\n",
    "Or in other words, each prediction divided by the sum of the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How would you assess whether a relationship is actually linear?\n",
    "1. Imagine that when you loaded your data, you unwittingly loaded each row of the data (both $X$ and $y$) twice and performed the same regression.  What is the effect on your estimates $\\beta$?\n",
    "1. Imagine when you loaded your data, you unwittingly loaded each column of the features (just $X$) twice and performed the same regression.  What is the effect on your estimates $\\beta$?\n",
    "1. Everything we've talked about so far involves loading all the data into memory.  What if you have more data than you can fit into memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exit Tickets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Enumerate the similarities and differences between linear regression and logistic regression.\n",
    "1. What is the purpose of splitting a data set into testing and training data sets?\n",
    "1. Why is it important to randomize a data set before splitting it into test and training data sets?\n",
    "1. What are the benefits of regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To assess if the relationship is linear, plot the distribution of the residuals as a function of $x$.  If there's a systematic bias, take a look at it and see what's going on.\n",
    "1. Loading rows twice has no effect on $\\beta$ but it does artificially increase your confidence (dividing it by a factor $\\sqrt{2}$)\n",
    "1. The problem becomes degenerate and $\\beta_j$ is now split between $\\beta_{j'}$ and $\\beta_{j''}$ such that $\\beta_j = \\beta_{j'} + \\beta_{j''}$.\n",
    "1. All of these problems can be solved using gradient descent, which only requires a *stream* of data, rather than the entire data set.  Linear regression (with either $L^2$, Huber penalty, epsilon insensitive) can be solved using `sklearn.linear_model.SGDRegressor` and logistic regression can be solved using `sklearn.linear_model.SGDClassifier`.  These methods implement a `partial_fit` method, which can iteratively updates the coefficients on small chunks of data.  In this case, you are no longer ram constrained, but constrained in the amount of time it takes to read data from disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2017 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
